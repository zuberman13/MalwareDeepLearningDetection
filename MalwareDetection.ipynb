{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "MalwareDetection",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalsadi/MalwareDeepLearningDetection/blob/master/MalwareDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1lCK3oajsi0",
        "colab_type": "text"
      },
      "source": [
        "# **Malware Classification Using Deep Learning**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbGYpeYhuwK2",
        "colab_type": "text"
      },
      "source": [
        "This notebook will attempt to utilze state of the art neural network architectures to further develop the accuracy achieved by researchers on this topic. Dataset information and network descriptions will be provided below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uria_HHzoGF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras \n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIWyLUVQkAdW",
        "colab_type": "text"
      },
      "source": [
        "## **Dataset**\n",
        "The malware dataset was obtained courtsey of Vision Research Lab\n",
        "University of California, Santa Barbara "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knjbVN7-obnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = np.load('/content/malimg.npz',allow_pickle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNV9eGoxo7PD",
        "colab_type": "code",
        "outputId": "4383a070-f573-4085-f685-6e80bea98c9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "BATCH_SIZE = 256 \n",
        "CELL_SIZE = 256 \n",
        "DROPOUT_RATE = 0.85 \n",
        "LEARNING_RATE = 1e-3 \n",
        "NODE_SIZE = [512, 256, 128] \n",
        "NUM_LAYERS = 5\n",
        "\n",
        "features = dataset['arr'][:, 0]\n",
        "features = np.array([feature for feature in features])\n",
        "features = np.reshape(features, (features.shape[0], features.shape[1] * features.shape[2]))\n",
        "r, c = features.shape\n",
        "\n",
        "print(\"Number of Samples\" , r)\n",
        "print(\"Number of Features\" , c)\n",
        "\n",
        "if 1==1:\n",
        "    features = StandardScaler().fit_transform(features)\n",
        "\n",
        "    \n",
        "labels = dataset['arr'][:, 1]\n",
        "labels = np.array([label for label in labels])\n",
        "\n",
        "\n",
        "\n",
        "one_hot = np.zeros((labels.shape[0], labels.max() + 1))\n",
        "one_hot[np.arange(labels.shape[0]), labels] = 1\n",
        "labels = one_hot\n",
        "labels[labels == 0] = 0\n",
        "num_features = features.shape[1]\n",
        "num_classes = labels.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "Y = labels\n",
        "X = features\n",
        "\n",
        "print(\"Shape of Labels\" , Y.shape)\n",
        "print(\"Shape of Features\" , X.shape)\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.01,\n",
        "                                                                                stratify=labels)#10% Test size\n",
        "\n",
        "train_size = int(train_features.shape[0])\n",
        "train_features = train_features[:train_size-(train_size % BATCH_SIZE)]\n",
        "train_labels = train_labels[:train_size-(train_size % BATCH_SIZE)]\n",
        "\n",
        "test_size = int(test_features.shape[0])\n",
        "test_features = test_features[:test_size - (test_size % BATCH_SIZE)]\n",
        "test_labels = test_labels[:test_size - (test_size % BATCH_SIZE)]\n",
        "\n",
        "\n",
        "\n",
        "r, c = train_features.shape\n",
        "print(\"Number of Training Samples\" , r)\n",
        "print(\"Number of Training Features\" , c)\n",
        "\n",
        "r, c = test_features.shape\n",
        "print(\"Number of Test Samples\" , r)\n",
        "print(\"Number of Test Features\" , c)\n",
        "#print(train_labels.shape)\n",
        "\n",
        "#print(tf.reshape(test_features[1], [32,32]))\n",
        "\n",
        "#print(train_labels)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples 9339\n",
            "Number of Features 1024\n",
            "Shape of Labels (9339, 25)\n",
            "Shape of Features (9339, 1024)\n",
            "Number of Training Samples 9216\n",
            "Number of Training Features 1024\n",
            "Number of Test Samples 0\n",
            "Number of Test Features 1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sCIo80sTqxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras as k\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "import sys\n",
        "  \n",
        "train_X = train_features.reshape(-1, 32,32, 1)\n",
        "test_X = test_features.reshape(-1, 32,32, 1)  \n",
        "Unchanined = X.reshape(-1, 32,32, 1)\n",
        "\n",
        "\n",
        "batch_size = 256#64\n",
        "epochs = 20\n",
        "num_classes = 25\n",
        "\n",
        "input_shape = (32, 32, 1)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
        "for i in range(1, 5):\n",
        "  model.add(Conv2D(i*32, (2, 2), activation='relu')) #64\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRuTA5kyUcxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def specificity(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    param:\n",
        "    y_pred - Predicted labels\n",
        "    y_true - True labels \n",
        "    Returns:\n",
        "    Specificity score\n",
        "    \"\"\"\n",
        "    neg_y_true = 1 - y_true\n",
        "    neg_y_pred = 1 - y_pred\n",
        "    fp = np.sum(neg_y_true * y_pred)\n",
        "    tn = np.sum(neg_y_true * neg_y_pred)\n",
        "    specificity = tn / (tn + fp + sys.float_info.epsilon)\n",
        "    return specificity\n",
        "\n",
        "def FPR(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    param:\n",
        "    y_pred - Predicted labels\n",
        "    y_true - True labels \n",
        "    Returns:\n",
        "    Specificity score\n",
        "    \"\"\"\n",
        "    neg_y_true = 1 - y_true\n",
        "    neg_y_pred = 1 - y_pred\n",
        "    fp = np.sum(neg_y_true * y_pred)\n",
        "    tn = np.sum(neg_y_true * neg_y_pred)\n",
        "    specificity = tn / (tn + fp + sys.float_info.epsilon)\n",
        "    return  1 - specificity\n",
        "  \n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=k.optimizers.Adam(),metrics=['accuracy',specificity, FPR])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IPz1kb6x2Q6",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB8M5vvczLgx",
        "colab_type": "text"
      },
      "source": [
        "### First fold\n",
        "  Automatic validation with 10% split\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PcOuV-HUeSY",
        "colab_type": "code",
        "outputId": "1cb0f04c-0f59-4a28-dbb3-5df54fe176d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "#History = model.fit(train_X, train_labels, batch_size=batch_size,epochs=20,verbose=1,validation_data=(test_X, test_labels))\n",
        "\n",
        "## Three folds on the model ~ Manual Kfold approach and add validation split for automatic cross validation\n",
        "History = model.fit(train_X, train_labels, batch_size=batch_size,epochs=100,verbose=1,validation_split=0.10)\n",
        "#History = model.fit(train_X, train_labels, batch_size=batch_size,epochs=150,verbose=1,validation_split=0.10)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8294 samples, validate on 922 samples\n",
            "Epoch 1/100\n",
            "8294/8294 [==============================] - 5s 659us/step - loss: 2.0428 - acc: 0.4088 - specificity: 0.9306 - FPR: 0.0694 - val_loss: 1.3752 - val_acc: 0.5672 - val_specificity: 0.9377 - val_FPR: 0.0623\n",
            "Epoch 2/100\n",
            "8294/8294 [==============================] - 4s 516us/step - loss: 1.1227 - acc: 0.6227 - specificity: 0.9401 - FPR: 0.0599 - val_loss: 0.8475 - val_acc: 0.7321 - val_specificity: 0.9428 - val_FPR: 0.0572\n",
            "Epoch 3/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.8225 - acc: 0.7188 - specificity: 0.9443 - FPR: 0.0557 - val_loss: 0.6827 - val_acc: 0.7744 - val_specificity: 0.9456 - val_FPR: 0.0544\n",
            "Epoch 4/100\n",
            "8294/8294 [==============================] - 4s 512us/step - loss: 0.6736 - acc: 0.7704 - specificity: 0.9468 - FPR: 0.0532 - val_loss: 0.5855 - val_acc: 0.7972 - val_specificity: 0.9474 - val_FPR: 0.0526\n",
            "Epoch 5/100\n",
            "8294/8294 [==============================] - 4s 514us/step - loss: 0.5763 - acc: 0.8032 - specificity: 0.9485 - FPR: 0.0515 - val_loss: 0.5061 - val_acc: 0.8286 - val_specificity: 0.9490 - val_FPR: 0.0510\n",
            "Epoch 6/100\n",
            "8294/8294 [==============================] - 4s 515us/step - loss: 0.5119 - acc: 0.8220 - specificity: 0.9495 - FPR: 0.0505 - val_loss: 0.4947 - val_acc: 0.8416 - val_specificity: 0.9499 - val_FPR: 0.0501\n",
            "Epoch 7/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.4441 - acc: 0.8406 - specificity: 0.9507 - FPR: 0.0493 - val_loss: 0.4486 - val_acc: 0.8633 - val_specificity: 0.9501 - val_FPR: 0.0499\n",
            "Epoch 8/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.4083 - acc: 0.8529 - specificity: 0.9512 - FPR: 0.0488 - val_loss: 0.5468 - val_acc: 0.8124 - val_specificity: 0.9510 - val_FPR: 0.0490\n",
            "Epoch 9/100\n",
            "8294/8294 [==============================] - 4s 514us/step - loss: 0.3858 - acc: 0.8627 - specificity: 0.9518 - FPR: 0.0482 - val_loss: 0.4380 - val_acc: 0.8547 - val_specificity: 0.9514 - val_FPR: 0.0486\n",
            "Epoch 10/100\n",
            "8294/8294 [==============================] - 4s 514us/step - loss: 0.3607 - acc: 0.8726 - specificity: 0.9521 - FPR: 0.0479 - val_loss: 0.4725 - val_acc: 0.8406 - val_specificity: 0.9517 - val_FPR: 0.0483\n",
            "Epoch 11/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.3275 - acc: 0.8818 - specificity: 0.9529 - FPR: 0.0471 - val_loss: 0.3966 - val_acc: 0.8807 - val_specificity: 0.9520 - val_FPR: 0.0480\n",
            "Epoch 12/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.3075 - acc: 0.8874 - specificity: 0.9531 - FPR: 0.0469 - val_loss: 0.4217 - val_acc: 0.8785 - val_specificity: 0.9525 - val_FPR: 0.0475\n",
            "Epoch 13/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.2798 - acc: 0.9007 - specificity: 0.9538 - FPR: 0.0462 - val_loss: 0.3845 - val_acc: 0.8850 - val_specificity: 0.9526 - val_FPR: 0.0474\n",
            "Epoch 14/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.2584 - acc: 0.9081 - specificity: 0.9542 - FPR: 0.0458 - val_loss: 0.3912 - val_acc: 0.8829 - val_specificity: 0.9532 - val_FPR: 0.0468\n",
            "Epoch 15/100\n",
            "8294/8294 [==============================] - 4s 512us/step - loss: 0.2432 - acc: 0.9157 - specificity: 0.9545 - FPR: 0.0455 - val_loss: 0.4092 - val_acc: 0.8872 - val_specificity: 0.9534 - val_FPR: 0.0466\n",
            "Epoch 16/100\n",
            "8294/8294 [==============================] - 4s 512us/step - loss: 0.2234 - acc: 0.9183 - specificity: 0.9549 - FPR: 0.0451 - val_loss: 0.4289 - val_acc: 0.8818 - val_specificity: 0.9533 - val_FPR: 0.0467\n",
            "Epoch 17/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.2132 - acc: 0.9251 - specificity: 0.9552 - FPR: 0.0448 - val_loss: 0.3711 - val_acc: 0.8839 - val_specificity: 0.9532 - val_FPR: 0.0468\n",
            "Epoch 18/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.2258 - acc: 0.9198 - specificity: 0.9549 - FPR: 0.0451 - val_loss: 0.4896 - val_acc: 0.8709 - val_specificity: 0.9535 - val_FPR: 0.0465\n",
            "Epoch 19/100\n",
            "8294/8294 [==============================] - 4s 512us/step - loss: 0.2139 - acc: 0.9246 - specificity: 0.9552 - FPR: 0.0448 - val_loss: 0.3937 - val_acc: 0.8905 - val_specificity: 0.9535 - val_FPR: 0.0465\n",
            "Epoch 20/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.2015 - acc: 0.9280 - specificity: 0.9555 - FPR: 0.0445 - val_loss: 0.4149 - val_acc: 0.8742 - val_specificity: 0.9534 - val_FPR: 0.0466\n",
            "Epoch 21/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.2124 - acc: 0.9226 - specificity: 0.9552 - FPR: 0.0448 - val_loss: 0.4481 - val_acc: 0.8872 - val_specificity: 0.9537 - val_FPR: 0.0463\n",
            "Epoch 22/100\n",
            "8294/8294 [==============================] - 4s 523us/step - loss: 0.1965 - acc: 0.9287 - specificity: 0.9556 - FPR: 0.0444 - val_loss: 0.4066 - val_acc: 0.8883 - val_specificity: 0.9540 - val_FPR: 0.0460\n",
            "Epoch 23/100\n",
            "8294/8294 [==============================] - 4s 518us/step - loss: 0.1763 - acc: 0.9333 - specificity: 0.9559 - FPR: 0.0441 - val_loss: 0.4298 - val_acc: 0.8915 - val_specificity: 0.9541 - val_FPR: 0.0459\n",
            "Epoch 24/100\n",
            "8294/8294 [==============================] - 4s 514us/step - loss: 0.1761 - acc: 0.9369 - specificity: 0.9561 - FPR: 0.0439 - val_loss: 0.4000 - val_acc: 0.8980 - val_specificity: 0.9540 - val_FPR: 0.0460\n",
            "Epoch 25/100\n",
            "8294/8294 [==============================] - 4s 516us/step - loss: 0.1736 - acc: 0.9386 - specificity: 0.9561 - FPR: 0.0439 - val_loss: 0.3952 - val_acc: 0.9024 - val_specificity: 0.9545 - val_FPR: 0.0455\n",
            "Epoch 26/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.1568 - acc: 0.9457 - specificity: 0.9565 - FPR: 0.0435 - val_loss: 0.4043 - val_acc: 0.8980 - val_specificity: 0.9544 - val_FPR: 0.0456\n",
            "Epoch 27/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.1618 - acc: 0.9421 - specificity: 0.9563 - FPR: 0.0437 - val_loss: 0.4428 - val_acc: 0.8894 - val_specificity: 0.9544 - val_FPR: 0.0456\n",
            "Epoch 28/100\n",
            "8294/8294 [==============================] - 4s 515us/step - loss: 0.1429 - acc: 0.9519 - specificity: 0.9568 - FPR: 0.0432 - val_loss: 0.4374 - val_acc: 0.8991 - val_specificity: 0.9547 - val_FPR: 0.0453\n",
            "Epoch 29/100\n",
            "8294/8294 [==============================] - 4s 512us/step - loss: 0.1573 - acc: 0.9426 - specificity: 0.9565 - FPR: 0.0435 - val_loss: 0.4428 - val_acc: 0.8915 - val_specificity: 0.9545 - val_FPR: 0.0455\n",
            "Epoch 30/100\n",
            "8294/8294 [==============================] - 4s 512us/step - loss: 0.1620 - acc: 0.9441 - specificity: 0.9565 - FPR: 0.0435 - val_loss: 0.3928 - val_acc: 0.9002 - val_specificity: 0.9546 - val_FPR: 0.0454\n",
            "Epoch 31/100\n",
            "8294/8294 [==============================] - 4s 512us/step - loss: 0.1594 - acc: 0.9451 - specificity: 0.9565 - FPR: 0.0435 - val_loss: 0.4636 - val_acc: 0.8991 - val_specificity: 0.9546 - val_FPR: 0.0454\n",
            "Epoch 32/100\n",
            "8294/8294 [==============================] - 4s 515us/step - loss: 0.1408 - acc: 0.9496 - specificity: 0.9569 - FPR: 0.0431 - val_loss: 0.5102 - val_acc: 0.8980 - val_specificity: 0.9548 - val_FPR: 0.0452\n",
            "Epoch 33/100\n",
            "8294/8294 [==============================] - 4s 515us/step - loss: 0.1450 - acc: 0.9492 - specificity: 0.9567 - FPR: 0.0433 - val_loss: 0.4176 - val_acc: 0.9035 - val_specificity: 0.9548 - val_FPR: 0.0452\n",
            "Epoch 34/100\n",
            "8294/8294 [==============================] - 4s 514us/step - loss: 0.1304 - acc: 0.9547 - specificity: 0.9570 - FPR: 0.0430 - val_loss: 0.4432 - val_acc: 0.9013 - val_specificity: 0.9550 - val_FPR: 0.0450\n",
            "Epoch 35/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.1334 - acc: 0.9550 - specificity: 0.9571 - FPR: 0.0429 - val_loss: 0.4484 - val_acc: 0.8905 - val_specificity: 0.9545 - val_FPR: 0.0455\n",
            "Epoch 36/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.1265 - acc: 0.9548 - specificity: 0.9572 - FPR: 0.0428 - val_loss: 0.4624 - val_acc: 0.8970 - val_specificity: 0.9551 - val_FPR: 0.0449\n",
            "Epoch 37/100\n",
            "8294/8294 [==============================] - 4s 508us/step - loss: 0.1321 - acc: 0.9539 - specificity: 0.9571 - FPR: 0.0429 - val_loss: 0.4186 - val_acc: 0.9046 - val_specificity: 0.9550 - val_FPR: 0.0450\n",
            "Epoch 38/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.1223 - acc: 0.9576 - specificity: 0.9572 - FPR: 0.0428 - val_loss: 0.4485 - val_acc: 0.9078 - val_specificity: 0.9553 - val_FPR: 0.0447\n",
            "Epoch 39/100\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.1272 - acc: 0.9567 - specificity: 0.9572 - FPR: 0.0428 - val_loss: 0.4717 - val_acc: 0.8959 - val_specificity: 0.9548 - val_FPR: 0.0452\n",
            "Epoch 40/100\n",
            "8294/8294 [==============================] - 4s 516us/step - loss: 0.1207 - acc: 0.9594 - specificity: 0.9573 - FPR: 0.0427 - val_loss: 0.4367 - val_acc: 0.8959 - val_specificity: 0.9547 - val_FPR: 0.0453\n",
            "Epoch 41/100\n",
            "8294/8294 [==============================] - 4s 514us/step - loss: 0.1147 - acc: 0.9602 - specificity: 0.9574 - FPR: 0.0426 - val_loss: 0.4206 - val_acc: 0.9111 - val_specificity: 0.9555 - val_FPR: 0.0445\n",
            "Epoch 42/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.1149 - acc: 0.9595 - specificity: 0.9575 - FPR: 0.0425 - val_loss: 0.4147 - val_acc: 0.9121 - val_specificity: 0.9552 - val_FPR: 0.0448\n",
            "Epoch 43/100\n",
            "8294/8294 [==============================] - 4s 514us/step - loss: 0.1179 - acc: 0.9590 - specificity: 0.9573 - FPR: 0.0427 - val_loss: 0.4600 - val_acc: 0.9067 - val_specificity: 0.9549 - val_FPR: 0.0451\n",
            "Epoch 44/100\n",
            "8294/8294 [==============================] - 4s 508us/step - loss: 0.1266 - acc: 0.9571 - specificity: 0.9573 - FPR: 0.0427 - val_loss: 0.3993 - val_acc: 0.9111 - val_specificity: 0.9555 - val_FPR: 0.0445\n",
            "Epoch 45/100\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.1152 - acc: 0.9592 - specificity: 0.9575 - FPR: 0.0425 - val_loss: 0.4201 - val_acc: 0.9121 - val_specificity: 0.9553 - val_FPR: 0.0447\n",
            "Epoch 46/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.1196 - acc: 0.9588 - specificity: 0.9574 - FPR: 0.0426 - val_loss: 0.4211 - val_acc: 0.9132 - val_specificity: 0.9551 - val_FPR: 0.0449\n",
            "Epoch 47/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.1162 - acc: 0.9615 - specificity: 0.9575 - FPR: 0.0425 - val_loss: 0.4120 - val_acc: 0.9089 - val_specificity: 0.9553 - val_FPR: 0.0447\n",
            "Epoch 48/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.1127 - acc: 0.9617 - specificity: 0.9576 - FPR: 0.0424 - val_loss: 0.3968 - val_acc: 0.9154 - val_specificity: 0.9555 - val_FPR: 0.0445\n",
            "Epoch 49/100\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.1098 - acc: 0.9623 - specificity: 0.9576 - FPR: 0.0424 - val_loss: 0.4347 - val_acc: 0.9089 - val_specificity: 0.9553 - val_FPR: 0.0447\n",
            "Epoch 50/100\n",
            "8294/8294 [==============================] - 4s 514us/step - loss: 0.1070 - acc: 0.9643 - specificity: 0.9576 - FPR: 0.0424 - val_loss: 0.4290 - val_acc: 0.9067 - val_specificity: 0.9553 - val_FPR: 0.0447\n",
            "Epoch 51/100\n",
            "8294/8294 [==============================] - 4s 514us/step - loss: 0.1050 - acc: 0.9647 - specificity: 0.9577 - FPR: 0.0423 - val_loss: 0.4465 - val_acc: 0.9132 - val_specificity: 0.9555 - val_FPR: 0.0445\n",
            "Epoch 52/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.1017 - acc: 0.9652 - specificity: 0.9577 - FPR: 0.0423 - val_loss: 0.4048 - val_acc: 0.9219 - val_specificity: 0.9560 - val_FPR: 0.0440\n",
            "Epoch 53/100\n",
            "8294/8294 [==============================] - 4s 512us/step - loss: 0.1064 - acc: 0.9627 - specificity: 0.9577 - FPR: 0.0423 - val_loss: 0.4015 - val_acc: 0.9089 - val_specificity: 0.9556 - val_FPR: 0.0444\n",
            "Epoch 54/100\n",
            "8294/8294 [==============================] - 4s 512us/step - loss: 0.1076 - acc: 0.9629 - specificity: 0.9577 - FPR: 0.0423 - val_loss: 0.3819 - val_acc: 0.9143 - val_specificity: 0.9556 - val_FPR: 0.0444\n",
            "Epoch 55/100\n",
            "8294/8294 [==============================] - 4s 515us/step - loss: 0.1010 - acc: 0.9668 - specificity: 0.9578 - FPR: 0.0422 - val_loss: 0.4310 - val_acc: 0.9089 - val_specificity: 0.9556 - val_FPR: 0.0444\n",
            "Epoch 56/100\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.0989 - acc: 0.9672 - specificity: 0.9578 - FPR: 0.0422 - val_loss: 0.3902 - val_acc: 0.9132 - val_specificity: 0.9556 - val_FPR: 0.0444\n",
            "Epoch 57/100\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.0956 - acc: 0.9678 - specificity: 0.9579 - FPR: 0.0421 - val_loss: 0.4269 - val_acc: 0.9187 - val_specificity: 0.9558 - val_FPR: 0.0442\n",
            "Epoch 58/100\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0996 - acc: 0.9650 - specificity: 0.9579 - FPR: 0.0421 - val_loss: 0.5139 - val_acc: 0.9056 - val_specificity: 0.9557 - val_FPR: 0.0443\n",
            "Epoch 59/100\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.1095 - acc: 0.9635 - specificity: 0.9576 - FPR: 0.0424 - val_loss: 0.3826 - val_acc: 0.9154 - val_specificity: 0.9556 - val_FPR: 0.0444\n",
            "Epoch 60/100\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.1015 - acc: 0.9642 - specificity: 0.9578 - FPR: 0.0422 - val_loss: 0.3837 - val_acc: 0.9100 - val_specificity: 0.9556 - val_FPR: 0.0444\n",
            "Epoch 61/100\n",
            "8294/8294 [==============================] - 4s 508us/step - loss: 0.0990 - acc: 0.9658 - specificity: 0.9579 - FPR: 0.0421 - val_loss: 0.4013 - val_acc: 0.9100 - val_specificity: 0.9556 - val_FPR: 0.0444\n",
            "Epoch 62/100\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0918 - acc: 0.9683 - specificity: 0.9580 - FPR: 0.0420 - val_loss: 0.3908 - val_acc: 0.9154 - val_specificity: 0.9559 - val_FPR: 0.0441\n",
            "Epoch 63/100\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.1001 - acc: 0.9665 - specificity: 0.9579 - FPR: 0.0421 - val_loss: 0.4169 - val_acc: 0.9143 - val_specificity: 0.9559 - val_FPR: 0.0441\n",
            "Epoch 64/100\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.1046 - acc: 0.9652 - specificity: 0.9578 - FPR: 0.0422 - val_loss: 0.3634 - val_acc: 0.9154 - val_specificity: 0.9557 - val_FPR: 0.0443\n",
            "Epoch 65/100\n",
            "8294/8294 [==============================] - 4s 520us/step - loss: 0.0956 - acc: 0.9674 - specificity: 0.9579 - FPR: 0.0421 - val_loss: 0.3988 - val_acc: 0.9143 - val_specificity: 0.9556 - val_FPR: 0.0444\n",
            "Epoch 66/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.0951 - acc: 0.9672 - specificity: 0.9580 - FPR: 0.0420 - val_loss: 0.4309 - val_acc: 0.9100 - val_specificity: 0.9557 - val_FPR: 0.0443\n",
            "Epoch 67/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.0919 - acc: 0.9684 - specificity: 0.9579 - FPR: 0.0421 - val_loss: 0.3970 - val_acc: 0.9111 - val_specificity: 0.9557 - val_FPR: 0.0443\n",
            "Epoch 68/100\n",
            "8294/8294 [==============================] - 4s 508us/step - loss: 0.0919 - acc: 0.9705 - specificity: 0.9580 - FPR: 0.0420 - val_loss: 0.4705 - val_acc: 0.9143 - val_specificity: 0.9558 - val_FPR: 0.0442\n",
            "Epoch 69/100\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0897 - acc: 0.9703 - specificity: 0.9580 - FPR: 0.0420 - val_loss: 0.4167 - val_acc: 0.9111 - val_specificity: 0.9558 - val_FPR: 0.0442\n",
            "Epoch 70/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.0882 - acc: 0.9719 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.4073 - val_acc: 0.9143 - val_specificity: 0.9559 - val_FPR: 0.0441\n",
            "Epoch 71/100\n",
            "8294/8294 [==============================] - 4s 512us/step - loss: 0.0836 - acc: 0.9724 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.4689 - val_acc: 0.9143 - val_specificity: 0.9559 - val_FPR: 0.0441\n",
            "Epoch 72/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.0850 - acc: 0.9707 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.4591 - val_acc: 0.9165 - val_specificity: 0.9560 - val_FPR: 0.0440\n",
            "Epoch 73/100\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0850 - acc: 0.9718 - specificity: 0.9582 - FPR: 0.0418 - val_loss: 0.4094 - val_acc: 0.9121 - val_specificity: 0.9558 - val_FPR: 0.0442\n",
            "Epoch 74/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.0945 - acc: 0.9667 - specificity: 0.9579 - FPR: 0.0421 - val_loss: 0.4530 - val_acc: 0.9089 - val_specificity: 0.9556 - val_FPR: 0.0444\n",
            "Epoch 75/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.0872 - acc: 0.9705 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.4149 - val_acc: 0.9176 - val_specificity: 0.9560 - val_FPR: 0.0440\n",
            "Epoch 76/100\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0922 - acc: 0.9693 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.4785 - val_acc: 0.9078 - val_specificity: 0.9556 - val_FPR: 0.0444\n",
            "Epoch 77/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.0843 - acc: 0.9715 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.4629 - val_acc: 0.9208 - val_specificity: 0.9561 - val_FPR: 0.0439\n",
            "Epoch 78/100\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0749 - acc: 0.9748 - specificity: 0.9584 - FPR: 0.0416 - val_loss: 0.4034 - val_acc: 0.9197 - val_specificity: 0.9559 - val_FPR: 0.0441\n",
            "Epoch 79/100\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0889 - acc: 0.9697 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.3952 - val_acc: 0.9219 - val_specificity: 0.9559 - val_FPR: 0.0441\n",
            "Epoch 80/100\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0744 - acc: 0.9754 - specificity: 0.9583 - FPR: 0.0417 - val_loss: 0.4163 - val_acc: 0.9241 - val_specificity: 0.9562 - val_FPR: 0.0438\n",
            "Epoch 81/100\n",
            "8294/8294 [==============================] - 4s 508us/step - loss: 0.0829 - acc: 0.9702 - specificity: 0.9582 - FPR: 0.0418 - val_loss: 0.4367 - val_acc: 0.9208 - val_specificity: 0.9561 - val_FPR: 0.0439\n",
            "Epoch 82/100\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0765 - acc: 0.9725 - specificity: 0.9583 - FPR: 0.0417 - val_loss: 0.4669 - val_acc: 0.9187 - val_specificity: 0.9558 - val_FPR: 0.0442\n",
            "Epoch 83/100\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0817 - acc: 0.9715 - specificity: 0.9582 - FPR: 0.0418 - val_loss: 0.4063 - val_acc: 0.9219 - val_specificity: 0.9561 - val_FPR: 0.0439\n",
            "Epoch 84/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.0796 - acc: 0.9711 - specificity: 0.9583 - FPR: 0.0417 - val_loss: 0.4418 - val_acc: 0.9241 - val_specificity: 0.9563 - val_FPR: 0.0437\n",
            "Epoch 85/100\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0813 - acc: 0.9702 - specificity: 0.9583 - FPR: 0.0417 - val_loss: 0.4006 - val_acc: 0.9089 - val_specificity: 0.9559 - val_FPR: 0.0441\n",
            "Epoch 86/100\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.0675 - acc: 0.9743 - specificity: 0.9585 - FPR: 0.0415 - val_loss: 0.4114 - val_acc: 0.9284 - val_specificity: 0.9567 - val_FPR: 0.0433\n",
            "Epoch 87/100\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.0721 - acc: 0.9711 - specificity: 0.9584 - FPR: 0.0416 - val_loss: 0.4206 - val_acc: 0.9230 - val_specificity: 0.9566 - val_FPR: 0.0434\n",
            "Epoch 88/100\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0717 - acc: 0.9712 - specificity: 0.9584 - FPR: 0.0416 - val_loss: 0.3910 - val_acc: 0.9252 - val_specificity: 0.9567 - val_FPR: 0.0433\n",
            "Epoch 89/100\n",
            "8294/8294 [==============================] - 4s 514us/step - loss: 0.0636 - acc: 0.9784 - specificity: 0.9586 - FPR: 0.0414 - val_loss: 0.4133 - val_acc: 0.9241 - val_specificity: 0.9566 - val_FPR: 0.0434\n",
            "Epoch 90/100\n",
            "8294/8294 [==============================] - 4s 524us/step - loss: 0.0655 - acc: 0.9752 - specificity: 0.9586 - FPR: 0.0414 - val_loss: 0.4213 - val_acc: 0.9317 - val_specificity: 0.9566 - val_FPR: 0.0434\n",
            "Epoch 91/100\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.0677 - acc: 0.9765 - specificity: 0.9586 - FPR: 0.0414 - val_loss: 0.3627 - val_acc: 0.9306 - val_specificity: 0.9567 - val_FPR: 0.0433\n",
            "Epoch 92/100\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0530 - acc: 0.9832 - specificity: 0.9589 - FPR: 0.0411 - val_loss: 0.4380 - val_acc: 0.9306 - val_specificity: 0.9568 - val_FPR: 0.0432\n",
            "Epoch 93/100\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0569 - acc: 0.9796 - specificity: 0.9588 - FPR: 0.0412 - val_loss: 0.3444 - val_acc: 0.9382 - val_specificity: 0.9570 - val_FPR: 0.0430\n",
            "Epoch 94/100\n",
            "8294/8294 [==============================] - 4s 508us/step - loss: 0.0559 - acc: 0.9799 - specificity: 0.9588 - FPR: 0.0412 - val_loss: 0.4083 - val_acc: 0.9338 - val_specificity: 0.9570 - val_FPR: 0.0430\n",
            "Epoch 95/100\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0541 - acc: 0.9823 - specificity: 0.9589 - FPR: 0.0411 - val_loss: 0.3522 - val_acc: 0.9306 - val_specificity: 0.9570 - val_FPR: 0.0430\n",
            "Epoch 96/100\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.0512 - acc: 0.9822 - specificity: 0.9589 - FPR: 0.0411 - val_loss: 0.3580 - val_acc: 0.9393 - val_specificity: 0.9571 - val_FPR: 0.0429\n",
            "Epoch 97/100\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0554 - acc: 0.9812 - specificity: 0.9589 - FPR: 0.0411 - val_loss: 0.4101 - val_acc: 0.9295 - val_specificity: 0.9569 - val_FPR: 0.0431\n",
            "Epoch 98/100\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0462 - acc: 0.9837 - specificity: 0.9590 - FPR: 0.0410 - val_loss: 0.4018 - val_acc: 0.9317 - val_specificity: 0.9570 - val_FPR: 0.0430\n",
            "Epoch 99/100\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0390 - acc: 0.9864 - specificity: 0.9592 - FPR: 0.0408 - val_loss: 0.4492 - val_acc: 0.9317 - val_specificity: 0.9570 - val_FPR: 0.0430\n",
            "Epoch 100/100\n",
            "8294/8294 [==============================] - 4s 508us/step - loss: 0.0374 - acc: 0.9870 - specificity: 0.9592 - FPR: 0.0408 - val_loss: 0.4434 - val_acc: 0.9328 - val_specificity: 0.9570 - val_FPR: 0.0430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otf-NOb3lpd3",
        "colab_type": "text"
      },
      "source": [
        "### Second fold\n",
        "1.   Data augumentation allows for artifically enlarged training set \n",
        "2.   Automatic learning rate reducation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_gscx0yjDNJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "318c95c0-8efa-4966-ba23-a090b22a2d63"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "datagen.fit(train_X)\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3,  verbose=1, factor=0.5,  min_lr=0.00001)\n",
        "\n",
        "History = model.fit(datagen.flow(train_X,train_labels, batch_size=batch_size),epochs=50,verbose=1,callbacks=[learning_rate_reduction])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 1.4985 - acc: 0.5853 - specificity: 0.9386 - FPR: 0.0614\n",
            "Epoch 2/50\n",
            " 1/36 [..............................] - ETA: 4s - loss: 0.9476 - acc: 0.7188 - specificity: 0.9425 - FPR: 0.0575"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1379: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc,specificity,FPR,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "36/36 [==============================] - 5s 128ms/step - loss: 0.9352 - acc: 0.6993 - specificity: 0.9431 - FPR: 0.0569\n",
            "Epoch 3/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.8272 - acc: 0.7250 - specificity: 0.9446 - FPR: 0.0554\n",
            "Epoch 4/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.7574 - acc: 0.7393 - specificity: 0.9456 - FPR: 0.0544\n",
            "Epoch 5/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.7400 - acc: 0.7484 - specificity: 0.9458 - FPR: 0.0542\n",
            "Epoch 6/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.7109 - acc: 0.7592 - specificity: 0.9462 - FPR: 0.0538\n",
            "Epoch 7/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.6693 - acc: 0.7677 - specificity: 0.9469 - FPR: 0.0531\n",
            "Epoch 8/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.6411 - acc: 0.7778 - specificity: 0.9472 - FPR: 0.0528\n",
            "Epoch 9/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.6340 - acc: 0.7808 - specificity: 0.9475 - FPR: 0.0525\n",
            "Epoch 10/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.6369 - acc: 0.7727 - specificity: 0.9473 - FPR: 0.0527\n",
            "Epoch 11/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.6112 - acc: 0.7811 - specificity: 0.9478 - FPR: 0.0522\n",
            "Epoch 12/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.5818 - acc: 0.7897 - specificity: 0.9482 - FPR: 0.0518\n",
            "Epoch 13/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.5933 - acc: 0.7956 - specificity: 0.9481 - FPR: 0.0519\n",
            "Epoch 14/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.5943 - acc: 0.7922 - specificity: 0.9482 - FPR: 0.0518\n",
            "Epoch 15/50\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.5619 - acc: 0.8021 - specificity: 0.9486 - FPR: 0.0514\n",
            "Epoch 16/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.5710 - acc: 0.7985 - specificity: 0.9486 - FPR: 0.0514\n",
            "Epoch 17/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.5598 - acc: 0.8014 - specificity: 0.9487 - FPR: 0.0513\n",
            "Epoch 18/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.5586 - acc: 0.8022 - specificity: 0.9487 - FPR: 0.0513\n",
            "Epoch 19/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.5420 - acc: 0.8069 - specificity: 0.9490 - FPR: 0.0510\n",
            "Epoch 20/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.5340 - acc: 0.8083 - specificity: 0.9491 - FPR: 0.0509\n",
            "Epoch 21/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.5342 - acc: 0.8138 - specificity: 0.9492 - FPR: 0.0508\n",
            "Epoch 22/50\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.5333 - acc: 0.8075 - specificity: 0.9490 - FPR: 0.0510\n",
            "Epoch 23/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.5356 - acc: 0.8127 - specificity: 0.9492 - FPR: 0.0508\n",
            "Epoch 24/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.5135 - acc: 0.8186 - specificity: 0.9495 - FPR: 0.0505\n",
            "Epoch 25/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.5402 - acc: 0.8117 - specificity: 0.9492 - FPR: 0.0508\n",
            "Epoch 26/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.5108 - acc: 0.8188 - specificity: 0.9495 - FPR: 0.0505\n",
            "Epoch 27/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.5120 - acc: 0.8168 - specificity: 0.9495 - FPR: 0.0505\n",
            "Epoch 28/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.4939 - acc: 0.8186 - specificity: 0.9498 - FPR: 0.0502\n",
            "Epoch 29/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.4959 - acc: 0.8236 - specificity: 0.9498 - FPR: 0.0502\n",
            "Epoch 30/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.4845 - acc: 0.8317 - specificity: 0.9501 - FPR: 0.0499\n",
            "Epoch 31/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.4754 - acc: 0.8281 - specificity: 0.9502 - FPR: 0.0498\n",
            "Epoch 32/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.4923 - acc: 0.8223 - specificity: 0.9500 - FPR: 0.0500\n",
            "Epoch 33/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.4848 - acc: 0.8255 - specificity: 0.9500 - FPR: 0.0500\n",
            "Epoch 34/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.4713 - acc: 0.8316 - specificity: 0.9503 - FPR: 0.0497\n",
            "Epoch 35/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.4789 - acc: 0.8276 - specificity: 0.9502 - FPR: 0.0498\n",
            "Epoch 36/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.4808 - acc: 0.8269 - specificity: 0.9500 - FPR: 0.0500\n",
            "Epoch 37/50\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.4671 - acc: 0.8309 - specificity: 0.9503 - FPR: 0.0497\n",
            "Epoch 38/50\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.4670 - acc: 0.8364 - specificity: 0.9505 - FPR: 0.0495\n",
            "Epoch 39/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.4684 - acc: 0.8301 - specificity: 0.9503 - FPR: 0.0497\n",
            "Epoch 40/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.4474 - acc: 0.8367 - specificity: 0.9507 - FPR: 0.0493\n",
            "Epoch 41/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.4529 - acc: 0.8355 - specificity: 0.9507 - FPR: 0.0493\n",
            "Epoch 42/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.4577 - acc: 0.8366 - specificity: 0.9505 - FPR: 0.0495\n",
            "Epoch 43/50\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.4583 - acc: 0.8330 - specificity: 0.9505 - FPR: 0.0495\n",
            "Epoch 44/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.4664 - acc: 0.8302 - specificity: 0.9503 - FPR: 0.0497\n",
            "Epoch 45/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.4507 - acc: 0.8390 - specificity: 0.9507 - FPR: 0.0493\n",
            "Epoch 46/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.4417 - acc: 0.8373 - specificity: 0.9506 - FPR: 0.0494\n",
            "Epoch 47/50\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.4461 - acc: 0.8406 - specificity: 0.9508 - FPR: 0.0492\n",
            "Epoch 48/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.4461 - acc: 0.8430 - specificity: 0.9508 - FPR: 0.0492\n",
            "Epoch 49/50\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.4501 - acc: 0.8408 - specificity: 0.9507 - FPR: 0.0493\n",
            "Epoch 50/50\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.4364 - acc: 0.8408 - specificity: 0.9509 - FPR: 0.0491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INJyvaayy7Lv",
        "colab_type": "text"
      },
      "source": [
        "### Third fold\n",
        "1.   Validate with validation set with 10% split\n",
        "2.   Automatic learning rate reducation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3R4tK4Nn36r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2b28208-6b07-4083-c4bd-4be0b14fc03d"
      },
      "source": [
        "History = model.fit(train_X, train_labels, batch_size=batch_size,epochs=50,verbose=1,callbacks=[learning_rate_reduction],validation_split=0.10)\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8294 samples, validate on 922 samples\n",
            "Epoch 1/50\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.3044 - acc: 0.9087 - specificity: 0.9546 - FPR: 0.0454 - val_loss: 0.1686 - val_acc: 0.9512 - val_specificity: 0.9564 - val_FPR: 0.0436\n",
            "Epoch 2/50\n",
            "8294/8294 [==============================] - 4s 513us/step - loss: 0.1586 - acc: 0.9447 - specificity: 0.9567 - FPR: 0.0433 - val_loss: 0.1500 - val_acc: 0.9512 - val_specificity: 0.9571 - val_FPR: 0.0429\n",
            "Epoch 3/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.1349 - acc: 0.9558 - specificity: 0.9573 - FPR: 0.0427 - val_loss: 0.1491 - val_acc: 0.9534 - val_specificity: 0.9570 - val_FPR: 0.0430\n",
            "Epoch 4/50\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.1131 - acc: 0.9609 - specificity: 0.9575 - FPR: 0.0425 - val_loss: 0.1536 - val_acc: 0.9544 - val_specificity: 0.9574 - val_FPR: 0.0426\n",
            "Epoch 5/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0820 - acc: 0.9701 - specificity: 0.9582 - FPR: 0.0418 - val_loss: 0.1584 - val_acc: 0.9555 - val_specificity: 0.9576 - val_FPR: 0.0424\n",
            "Epoch 6/50\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.0717 - acc: 0.9756 - specificity: 0.9584 - FPR: 0.0416 - val_loss: 0.1745 - val_acc: 0.9523 - val_specificity: 0.9575 - val_FPR: 0.0425\n",
            "Epoch 7/50\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0692 - acc: 0.9748 - specificity: 0.9585 - FPR: 0.0415 - val_loss: 0.1691 - val_acc: 0.9534 - val_specificity: 0.9576 - val_FPR: 0.0424\n",
            "Epoch 8/50\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0574 - acc: 0.9779 - specificity: 0.9587 - FPR: 0.0413 - val_loss: 0.1838 - val_acc: 0.9512 - val_specificity: 0.9577 - val_FPR: 0.0423\n",
            "Epoch 9/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0523 - acc: 0.9832 - specificity: 0.9589 - FPR: 0.0411 - val_loss: 0.1904 - val_acc: 0.9512 - val_specificity: 0.9577 - val_FPR: 0.0423\n",
            "Epoch 10/50\n",
            "8294/8294 [==============================] - 4s 512us/step - loss: 0.0517 - acc: 0.9803 - specificity: 0.9589 - FPR: 0.0411 - val_loss: 0.1773 - val_acc: 0.9512 - val_specificity: 0.9578 - val_FPR: 0.0422\n",
            "Epoch 11/50\n",
            "8294/8294 [==============================] - 4s 506us/step - loss: 0.0418 - acc: 0.9858 - specificity: 0.9590 - FPR: 0.0410 - val_loss: 0.1794 - val_acc: 0.9534 - val_specificity: 0.9578 - val_FPR: 0.0422\n",
            "Epoch 12/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0409 - acc: 0.9863 - specificity: 0.9591 - FPR: 0.0409 - val_loss: 0.1991 - val_acc: 0.9534 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 13/50\n",
            "8294/8294 [==============================] - 4s 508us/step - loss: 0.0441 - acc: 0.9854 - specificity: 0.9591 - FPR: 0.0409 - val_loss: 0.2102 - val_acc: 0.9523 - val_specificity: 0.9578 - val_FPR: 0.0422\n",
            "Epoch 14/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0512 - acc: 0.9830 - specificity: 0.9589 - FPR: 0.0411 - val_loss: 0.1941 - val_acc: 0.9523 - val_specificity: 0.9578 - val_FPR: 0.0422\n",
            "Epoch 15/50\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.0365 - acc: 0.9884 - specificity: 0.9592 - FPR: 0.0408 - val_loss: 0.2113 - val_acc: 0.9479 - val_specificity: 0.9577 - val_FPR: 0.0423\n",
            "Epoch 16/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0352 - acc: 0.9885 - specificity: 0.9592 - FPR: 0.0408 - val_loss: 0.2095 - val_acc: 0.9523 - val_specificity: 0.9578 - val_FPR: 0.0422\n",
            "Epoch 17/50\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0360 - acc: 0.9871 - specificity: 0.9592 - FPR: 0.0408 - val_loss: 0.1869 - val_acc: 0.9544 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 18/50\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.0368 - acc: 0.9863 - specificity: 0.9592 - FPR: 0.0408 - val_loss: 0.2949 - val_acc: 0.9425 - val_specificity: 0.9574 - val_FPR: 0.0426\n",
            "Epoch 19/50\n",
            "8294/8294 [==============================] - 4s 508us/step - loss: 0.0449 - acc: 0.9834 - specificity: 0.9591 - FPR: 0.0409 - val_loss: 0.2021 - val_acc: 0.9447 - val_specificity: 0.9576 - val_FPR: 0.0424\n",
            "Epoch 20/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0335 - acc: 0.9875 - specificity: 0.9593 - FPR: 0.0407 - val_loss: 0.2299 - val_acc: 0.9534 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 21/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0383 - acc: 0.9876 - specificity: 0.9592 - FPR: 0.0408 - val_loss: 0.2556 - val_acc: 0.9523 - val_specificity: 0.9577 - val_FPR: 0.0423\n",
            "Epoch 22/50\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.0389 - acc: 0.9872 - specificity: 0.9592 - FPR: 0.0408 - val_loss: 0.2192 - val_acc: 0.9566 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 23/50\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.0308 - acc: 0.9891 - specificity: 0.9594 - FPR: 0.0406 - val_loss: 0.2260 - val_acc: 0.9566 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 24/50\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.0286 - acc: 0.9902 - specificity: 0.9594 - FPR: 0.0406 - val_loss: 0.2602 - val_acc: 0.9436 - val_specificity: 0.9576 - val_FPR: 0.0424\n",
            "Epoch 25/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0406 - acc: 0.9866 - specificity: 0.9592 - FPR: 0.0408 - val_loss: 0.2115 - val_acc: 0.9555 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 26/50\n",
            "8294/8294 [==============================] - 4s 505us/step - loss: 0.0253 - acc: 0.9916 - specificity: 0.9594 - FPR: 0.0406 - val_loss: 0.2458 - val_acc: 0.9479 - val_specificity: 0.9578 - val_FPR: 0.0422\n",
            "Epoch 27/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0274 - acc: 0.9912 - specificity: 0.9595 - FPR: 0.0405 - val_loss: 0.2668 - val_acc: 0.9501 - val_specificity: 0.9577 - val_FPR: 0.0423\n",
            "Epoch 28/50\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0367 - acc: 0.9878 - specificity: 0.9593 - FPR: 0.0407 - val_loss: 0.2462 - val_acc: 0.9501 - val_specificity: 0.9578 - val_FPR: 0.0422\n",
            "Epoch 29/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0260 - acc: 0.9914 - specificity: 0.9595 - FPR: 0.0405 - val_loss: 0.2199 - val_acc: 0.9566 - val_specificity: 0.9578 - val_FPR: 0.0422\n",
            "Epoch 30/50\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.0289 - acc: 0.9912 - specificity: 0.9594 - FPR: 0.0406 - val_loss: 0.2053 - val_acc: 0.9555 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 31/50\n",
            "8294/8294 [==============================] - 4s 506us/step - loss: 0.0290 - acc: 0.9898 - specificity: 0.9594 - FPR: 0.0406 - val_loss: 0.2010 - val_acc: 0.9588 - val_specificity: 0.9581 - val_FPR: 0.0419\n",
            "Epoch 32/50\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.0283 - acc: 0.9901 - specificity: 0.9594 - FPR: 0.0406 - val_loss: 0.2208 - val_acc: 0.9544 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 33/50\n",
            "8294/8294 [==============================] - 4s 508us/step - loss: 0.0315 - acc: 0.9887 - specificity: 0.9594 - FPR: 0.0406 - val_loss: 0.2125 - val_acc: 0.9588 - val_specificity: 0.9580 - val_FPR: 0.0420\n",
            "Epoch 34/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0224 - acc: 0.9925 - specificity: 0.9595 - FPR: 0.0405 - val_loss: 0.2388 - val_acc: 0.9566 - val_specificity: 0.9580 - val_FPR: 0.0420\n",
            "Epoch 35/50\n",
            "8294/8294 [==============================] - 4s 506us/step - loss: 0.0239 - acc: 0.9929 - specificity: 0.9595 - FPR: 0.0405 - val_loss: 0.2595 - val_acc: 0.9512 - val_specificity: 0.9578 - val_FPR: 0.0422\n",
            "Epoch 36/50\n",
            "8294/8294 [==============================] - 4s 508us/step - loss: 0.0238 - acc: 0.9917 - specificity: 0.9595 - FPR: 0.0405 - val_loss: 0.2426 - val_acc: 0.9588 - val_specificity: 0.9580 - val_FPR: 0.0420\n",
            "Epoch 37/50\n",
            "8294/8294 [==============================] - 4s 509us/step - loss: 0.0257 - acc: 0.9908 - specificity: 0.9595 - FPR: 0.0405 - val_loss: 0.2287 - val_acc: 0.9555 - val_specificity: 0.9580 - val_FPR: 0.0420\n",
            "Epoch 38/50\n",
            "8294/8294 [==============================] - 4s 505us/step - loss: 0.0268 - acc: 0.9911 - specificity: 0.9595 - FPR: 0.0405 - val_loss: 0.2286 - val_acc: 0.9523 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 39/50\n",
            "8294/8294 [==============================] - 4s 505us/step - loss: 0.0199 - acc: 0.9926 - specificity: 0.9596 - FPR: 0.0404 - val_loss: 0.2499 - val_acc: 0.9523 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 40/50\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.0219 - acc: 0.9925 - specificity: 0.9595 - FPR: 0.0405 - val_loss: 0.2866 - val_acc: 0.9534 - val_specificity: 0.9580 - val_FPR: 0.0420\n",
            "Epoch 41/50\n",
            "8294/8294 [==============================] - 4s 503us/step - loss: 0.0259 - acc: 0.9918 - specificity: 0.9595 - FPR: 0.0405 - val_loss: 0.2610 - val_acc: 0.9479 - val_specificity: 0.9578 - val_FPR: 0.0422\n",
            "Epoch 42/50\n",
            "8294/8294 [==============================] - 4s 506us/step - loss: 0.0218 - acc: 0.9919 - specificity: 0.9595 - FPR: 0.0405 - val_loss: 0.2274 - val_acc: 0.9544 - val_specificity: 0.9580 - val_FPR: 0.0420\n",
            "Epoch 43/50\n",
            "8294/8294 [==============================] - 4s 506us/step - loss: 0.0227 - acc: 0.9929 - specificity: 0.9596 - FPR: 0.0404 - val_loss: 0.2094 - val_acc: 0.9610 - val_specificity: 0.9581 - val_FPR: 0.0419\n",
            "Epoch 44/50\n",
            "8294/8294 [==============================] - 4s 507us/step - loss: 0.0233 - acc: 0.9925 - specificity: 0.9596 - FPR: 0.0404 - val_loss: 0.2479 - val_acc: 0.9555 - val_specificity: 0.9580 - val_FPR: 0.0420\n",
            "Epoch 45/50\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0309 - acc: 0.9901 - specificity: 0.9594 - FPR: 0.0406 - val_loss: 0.2260 - val_acc: 0.9555 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 46/50\n",
            "8294/8294 [==============================] - 4s 506us/step - loss: 0.0280 - acc: 0.9905 - specificity: 0.9594 - FPR: 0.0406 - val_loss: 0.2244 - val_acc: 0.9512 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 47/50\n",
            "8294/8294 [==============================] - 4s 506us/step - loss: 0.0217 - acc: 0.9929 - specificity: 0.9595 - FPR: 0.0405 - val_loss: 0.2415 - val_acc: 0.9566 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 48/50\n",
            "8294/8294 [==============================] - 4s 510us/step - loss: 0.0227 - acc: 0.9923 - specificity: 0.9596 - FPR: 0.0404 - val_loss: 0.2422 - val_acc: 0.9566 - val_specificity: 0.9580 - val_FPR: 0.0420\n",
            "Epoch 49/50\n",
            "8294/8294 [==============================] - 4s 511us/step - loss: 0.0215 - acc: 0.9928 - specificity: 0.9596 - FPR: 0.0404 - val_loss: 0.2254 - val_acc: 0.9523 - val_specificity: 0.9579 - val_FPR: 0.0421\n",
            "Epoch 50/50\n",
            "8294/8294 [==============================] - 4s 506us/step - loss: 0.0211 - acc: 0.9936 - specificity: 0.9596 - FPR: 0.0404 - val_loss: 0.2651 - val_acc: 0.9479 - val_specificity: 0.9578 - val_FPR: 0.0422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfWecsjkjZkU",
        "colab_type": "text"
      },
      "source": [
        "## **Performance Metrics**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yelpg-EQ6UlZ",
        "colab_type": "text"
      },
      "source": [
        "### **Accuracy and Precison**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcDT4TiPAF5N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e5f40833-8e0e-401b-8cf7-c2013e506519"
      },
      "source": [
        "print(\"Final Training Accuracy: \", History.history['acc'][-1])\n",
        "print(\"Final Testing Accuracy: \", History.history['val_acc'][-1])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Training Accuracy:  0.9936098380924737\n",
            "Final Testing Accuracy:  0.9479392582061751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HHzenfr9BxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.10,\n",
        "                                                                                stratify=labels)#10% Test size\n",
        "\n",
        "\n",
        "train_size = int(train_features.shape[0])\n",
        "train_features = train_features[:train_size-(train_size % BATCH_SIZE)]\n",
        "train_labels = train_labels[:train_size-(train_size % BATCH_SIZE)]\n",
        "\n",
        "test_size = int(test_features.shape[0])\n",
        "test_features = test_features[:test_size - (test_size % BATCH_SIZE)]\n",
        "test_labels = test_labels[:test_size - (test_size % BATCH_SIZE)]\n",
        "\n",
        "labels1 = test_labels\n",
        "testX = test_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWtH_WBk_-cz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e74cb0cf-15f6-40bf-98e8-ffbb1b00af07"
      },
      "source": [
        "print(testX.shape)\n",
        "print(labels1.shape)\n",
        "testX = testX.reshape(-1, 32,32, 1)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768, 1024)\n",
            "(768, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytpc51Xs6d8s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "b195b969-6003-4000-9a2a-308ba72b72b8"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n",
        "ypredq = model.predict(testX,batch_size=256)\n",
        "ypredq=np.argmax(ypredq, axis=1)\n",
        "test_lab = np.argmax(labels1, axis=1)\n",
        "print(classification_report(test_lab, ypredq))\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        11\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       0.99      1.00      0.99       244\n",
            "           3       1.00      0.99      1.00       131\n",
            "           4       1.00      1.00      1.00        14\n",
            "           5       1.00      1.00      1.00         8\n",
            "           6       1.00      1.00      1.00        16\n",
            "           7       1.00      0.93      0.96        14\n",
            "           8       1.00      1.00      1.00        17\n",
            "           9       1.00      1.00      1.00        13\n",
            "          10       1.00      1.00      1.00        33\n",
            "          11       1.00      1.00      1.00        34\n",
            "          12       1.00      1.00      1.00        13\n",
            "          13       1.00      1.00      1.00        15\n",
            "          14       1.00      1.00      1.00        10\n",
            "          15       1.00      1.00      1.00        13\n",
            "          16       1.00      0.90      0.95        10\n",
            "          17       1.00      1.00      1.00        12\n",
            "          18       1.00      1.00      1.00        14\n",
            "          19       1.00      1.00      1.00         7\n",
            "          20       1.00      1.00      1.00        11\n",
            "          21       1.00      1.00      1.00        11\n",
            "          22       1.00      1.00      1.00        36\n",
            "          23       1.00      1.00      1.00         8\n",
            "          24       1.00      1.00      1.00        64\n",
            "\n",
            "    accuracy                           1.00       768\n",
            "   macro avg       1.00      0.99      1.00       768\n",
            "weighted avg       1.00      1.00      1.00       768\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoAmn-g-jmXb",
        "colab_type": "text"
      },
      "source": [
        "### **True Positive Rate (TPR)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jZUs4ndpajB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8f13042f-cec3-4242-f554-9c3fc485468b"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(test_lab.shape)\n",
        "print(ypredq.shape)\n",
        "\n",
        "cm = confusion_matrix(test_lab,ypredq)\n",
        "print(np.diag(cm))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768,)\n",
            "(768,)\n",
            "[ 11   9 244 130  14   8  16  13  17  13  33  34  13  15  10  13   9  12\n",
            "  14   7  11  11  36   8  64]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJeYtIesloEZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "129e3a81-312d-4e73-e089-d7ffeab2421c"
      },
      "source": [
        "TotalTP , i = 0, 0\n",
        "while i <25:\n",
        "  TotalTP += np.diag(cm)[i]\n",
        "  i = i +1\n",
        "print(\"Total Number of Test Samples\",np.sum(cm))\n",
        "print(\"Number of True Positives\",TotalTP)\n",
        "print(\"Number of False Positives\",TotalTP)\n",
        "print(\"True Postive Rate\", TotalTP/np.sum(cm))\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of Test Samples 768\n",
            "Number of True Positives 765\n",
            "Number of False Positives 765\n",
            "True Postive Rate 0.99609375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4dQx0qi0cLm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fab5c294-d740-4136-e472-f84859236095"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(test_lab, ypredq,pos_label=1)\n",
        "metrics.auc(fpr, tpr)\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01449275362318836"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RirtxGpr4vqG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2e6a3f21-33a3-4281-8cae-bc32ee2f69df"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(tpr,fpr)\n",
        "plt.xlabel('True Positive Rate')\n",
        "plt.ylabel('False Positive Rate')\n",
        "plt.title('ROC Graph')\n",
        "plt.show()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMRQkSR1DQZLU+f+QndiarnHsbgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}