{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Malware Detection Malimg",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalsadi/MalwareDeepLearningDetection/blob/master/Malware_Detection_Malimg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1lCK3oajsi0",
        "colab_type": "text"
      },
      "source": [
        "# **Malware Classification Using Deep Learning**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbGYpeYhuwK2",
        "colab_type": "text"
      },
      "source": [
        "This notebook will attempt to utilze state of the art neural network architectures to further develop the accuracy achieved by researchers on this topic. Dataset information and network descriptions will be provided below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uria_HHzoGF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras \n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7EOkalP06aw",
        "colab_type": "text"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIWyLUVQkAdW",
        "colab_type": "text"
      },
      "source": [
        "## **Dataset**\n",
        "The malware dataset was obtained courtsey of Vision Research Lab\n",
        "University of California, Santa Barbara "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knjbVN7-obnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = np.load('/content/malimg.npz',allow_pickle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNV9eGoxo7PD",
        "colab_type": "code",
        "outputId": "b760f1ad-07a5-4bab-9d68-025efa46faf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "BATCH_SIZE = 256 \n",
        "CELL_SIZE = 256 \n",
        "DROPOUT_RATE = 0.85 \n",
        "LEARNING_RATE = 1e-3 \n",
        "NODE_SIZE = [512, 256, 128] \n",
        "NUM_LAYERS = 5\n",
        "\n",
        "features = dataset['arr'][:, 0]\n",
        "features = np.array([feature for feature in features])\n",
        "features = np.reshape(features, (features.shape[0], features.shape[1] * features.shape[2]))\n",
        "r, c = features.shape\n",
        "\n",
        "print(\"Number of Samples\" , r)\n",
        "print(\"Number of Features\" , c)\n",
        "\n",
        "if 1==1:\n",
        "    features = StandardScaler().fit_transform(features)\n",
        "\n",
        "    \n",
        "labels = dataset['arr'][:, 1]\n",
        "labels = np.array([label for label in labels])\n",
        "\n",
        "\n",
        "one_hot = np.zeros((labels.shape[0], labels.max() + 1))\n",
        "one_hot[np.arange(labels.shape[0]), labels] = 1\n",
        "labels = one_hot\n",
        "labels[labels == 0] = 0\n",
        "num_features = features.shape[1]\n",
        "num_classes = labels.shape[1]\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.10,\n",
        "                                                                                stratify=labels)\n",
        "\n",
        "train_size = int(train_features.shape[0])\n",
        "train_features = train_features[:train_size-(train_size % BATCH_SIZE)]\n",
        "train_labels = train_labels[:train_size-(train_size % BATCH_SIZE)]\n",
        "\n",
        "test_size = int(test_features.shape[0])\n",
        "test_features = test_features[:test_size - (test_size % BATCH_SIZE)]\n",
        "test_labels = test_labels[:test_size - (test_size % BATCH_SIZE)]\n",
        "\n",
        "\n",
        "\n",
        "r, c = train_features.shape\n",
        "print(\"Number of Training Samples\" , r)\n",
        "print(\"Number of Training Features\" , c)\n",
        "\n",
        "r, c = test_features.shape\n",
        "print(\"Number of Test Samples\" , r)\n",
        "print(\"Number of Test Features\" , c)\n",
        "print(train_labels.shape)\n",
        "\n",
        "print(tf.reshape(test_features[1], [32,32]))\n",
        "\n",
        "print(train_features.shape,test_features.shape, train_labels.shape, test_labels.shape )\n",
        "\n",
        "print(train_labels)\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples 9339\n",
            "Number of Features 1024\n",
            "Number of Training Samples 8192\n",
            "Number of Training Features 1024\n",
            "Number of Test Samples 768\n",
            "Number of Test Features 1024\n",
            "(8192, 25)\n",
            "Tensor(\"Reshape_10:0\", shape=(32, 32), dtype=float64)\n",
            "(8192, 1024) (768, 1024) (8192, 25) (768, 25)\n",
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sCIo80sTqxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras as k\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "import sys\n",
        "  \n",
        "train_X = train_features.reshape(-1, 32,32, 1)\n",
        "test_X = test_features.reshape(-1, 32,32, 1)  \n",
        "\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "num_classes = 25\n",
        "\n",
        "input_shape = (32, 32, 1)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRuTA5kyUcxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def specificity(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    param:\n",
        "    y_pred - Predicted labels\n",
        "    y_true - True labels \n",
        "    Returns:\n",
        "    Specificity score\n",
        "    \"\"\"\n",
        "    neg_y_true = 1 - y_true\n",
        "    neg_y_pred = 1 - y_pred\n",
        "    fp = np.sum(neg_y_true * y_pred)\n",
        "    tn = np.sum(neg_y_true * neg_y_pred)\n",
        "    specificity = tn / (tn + fp + sys.float_info.epsilon)\n",
        "    return specificity\n",
        "\n",
        "def FPR(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    param:\n",
        "    y_pred - Predicted labels\n",
        "    y_true - True labels \n",
        "    Returns:\n",
        "    Specificity score\n",
        "    \"\"\"\n",
        "    neg_y_true = 1 - y_true\n",
        "    neg_y_pred = 1 - y_pred\n",
        "    fp = np.sum(neg_y_true * y_pred)\n",
        "    tn = np.sum(neg_y_true * neg_y_pred)\n",
        "    specificity = tn / (tn + fp + sys.float_info.epsilon)\n",
        "    return  1 - specificity\n",
        "  \n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=k.optimizers.Adam(),metrics=['accuracy',specificity, FPR])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PcOuV-HUeSY",
        "colab_type": "code",
        "outputId": "9a311d42-760a-405a-d16c-eafe0c84fe69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "print(train_labels)\n",
        "print(test_labels)\n",
        "#20\n",
        "History = model.fit(train_X, train_labels, batch_size=batch_size,epochs=20,verbose=1,validation_data=(test_X, test_labels))"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Train on 8192 samples, validate on 768 samples\n",
            "Epoch 1/20\n",
            "8192/8192 [==============================] - 2s 294us/step - loss: 1.4183 - acc: 0.5664 - specificity: 0.9372 - FPR: 0.0628 - val_loss: 0.6658 - val_acc: 0.7708 - val_specificity: 0.9459 - val_FPR: 0.0541\n",
            "Epoch 2/20\n",
            "8192/8192 [==============================] - 1s 145us/step - loss: 0.7049 - acc: 0.7631 - specificity: 0.9460 - FPR: 0.0540 - val_loss: 0.4376 - val_acc: 0.8359 - val_specificity: 0.9500 - val_FPR: 0.0500\n",
            "Epoch 3/20\n",
            "8192/8192 [==============================] - 1s 140us/step - loss: 0.5103 - acc: 0.8153 - specificity: 0.9491 - FPR: 0.0509 - val_loss: 0.4098 - val_acc: 0.8164 - val_specificity: 0.9510 - val_FPR: 0.0490\n",
            "Epoch 4/20\n",
            "8192/8192 [==============================] - 1s 141us/step - loss: 0.4067 - acc: 0.8525 - specificity: 0.9510 - FPR: 0.0490 - val_loss: 0.3447 - val_acc: 0.8607 - val_specificity: 0.9522 - val_FPR: 0.0478\n",
            "Epoch 5/20\n",
            "8192/8192 [==============================] - 1s 141us/step - loss: 0.3320 - acc: 0.8752 - specificity: 0.9524 - FPR: 0.0476 - val_loss: 0.3527 - val_acc: 0.8398 - val_specificity: 0.9525 - val_FPR: 0.0475\n",
            "Epoch 6/20\n",
            "8192/8192 [==============================] - 1s 141us/step - loss: 0.2713 - acc: 0.9012 - specificity: 0.9537 - FPR: 0.0463 - val_loss: 0.4311 - val_acc: 0.8307 - val_specificity: 0.9528 - val_FPR: 0.0472\n",
            "Epoch 7/20\n",
            "8192/8192 [==============================] - 1s 140us/step - loss: 0.2092 - acc: 0.9224 - specificity: 0.9550 - FPR: 0.0450 - val_loss: 0.2981 - val_acc: 0.8828 - val_specificity: 0.9541 - val_FPR: 0.0459\n",
            "Epoch 8/20\n",
            "8192/8192 [==============================] - 1s 142us/step - loss: 0.1749 - acc: 0.9359 - specificity: 0.9558 - FPR: 0.0442 - val_loss: 0.3352 - val_acc: 0.8776 - val_specificity: 0.9545 - val_FPR: 0.0455\n",
            "Epoch 9/20\n",
            "8192/8192 [==============================] - 1s 140us/step - loss: 0.1454 - acc: 0.9476 - specificity: 0.9565 - FPR: 0.0435 - val_loss: 0.3690 - val_acc: 0.8841 - val_specificity: 0.9545 - val_FPR: 0.0455\n",
            "Epoch 10/20\n",
            "8192/8192 [==============================] - 1s 141us/step - loss: 0.1297 - acc: 0.9546 - specificity: 0.9569 - FPR: 0.0431 - val_loss: 0.3444 - val_acc: 0.8750 - val_specificity: 0.9544 - val_FPR: 0.0456\n",
            "Epoch 11/20\n",
            "8192/8192 [==============================] - 1s 139us/step - loss: 0.1168 - acc: 0.9601 - specificity: 0.9572 - FPR: 0.0428 - val_loss: 0.3648 - val_acc: 0.8789 - val_specificity: 0.9547 - val_FPR: 0.0453\n",
            "Epoch 12/20\n",
            "8192/8192 [==============================] - 1s 141us/step - loss: 0.1048 - acc: 0.9648 - specificity: 0.9576 - FPR: 0.0424 - val_loss: 0.3790 - val_acc: 0.8880 - val_specificity: 0.9547 - val_FPR: 0.0453\n",
            "Epoch 13/20\n",
            "8192/8192 [==============================] - 1s 144us/step - loss: 0.0972 - acc: 0.9642 - specificity: 0.9577 - FPR: 0.0423 - val_loss: 0.4025 - val_acc: 0.8841 - val_specificity: 0.9550 - val_FPR: 0.0450\n",
            "Epoch 14/20\n",
            "8192/8192 [==============================] - 1s 142us/step - loss: 0.0828 - acc: 0.9729 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.4858 - val_acc: 0.8776 - val_specificity: 0.9547 - val_FPR: 0.0453\n",
            "Epoch 15/20\n",
            "8192/8192 [==============================] - 1s 141us/step - loss: 0.0834 - acc: 0.9718 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.4109 - val_acc: 0.8737 - val_specificity: 0.9549 - val_FPR: 0.0451\n",
            "Epoch 16/20\n",
            "8192/8192 [==============================] - 1s 145us/step - loss: 0.0847 - acc: 0.9696 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.4908 - val_acc: 0.8841 - val_specificity: 0.9550 - val_FPR: 0.0450\n",
            "Epoch 17/20\n",
            "8192/8192 [==============================] - 1s 142us/step - loss: 0.0833 - acc: 0.9697 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.4105 - val_acc: 0.8789 - val_specificity: 0.9552 - val_FPR: 0.0448\n",
            "Epoch 18/20\n",
            "8192/8192 [==============================] - 1s 143us/step - loss: 0.0731 - acc: 0.9751 - specificity: 0.9583 - FPR: 0.0417 - val_loss: 0.4406 - val_acc: 0.8802 - val_specificity: 0.9551 - val_FPR: 0.0449\n",
            "Epoch 19/20\n",
            "8192/8192 [==============================] - 1s 144us/step - loss: 0.0844 - acc: 0.9705 - specificity: 0.9582 - FPR: 0.0418 - val_loss: 0.5948 - val_acc: 0.8659 - val_specificity: 0.9546 - val_FPR: 0.0454\n",
            "Epoch 20/20\n",
            "8192/8192 [==============================] - 1s 142us/step - loss: 0.0733 - acc: 0.9731 - specificity: 0.9584 - FPR: 0.0416 - val_loss: 0.4550 - val_acc: 0.8815 - val_specificity: 0.9551 - val_FPR: 0.0449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfWecsjkjZkU",
        "colab_type": "text"
      },
      "source": [
        "## **Performance Metrics**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yelpg-EQ6UlZ",
        "colab_type": "text"
      },
      "source": [
        "### **Accuracy and Precison**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcDT4TiPAF5N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6cea5ee0-565e-41f6-eb57-b6c02c7538b2"
      },
      "source": [
        "print(\"Final Training Accuracy: \", History.history['acc'][-1])\n",
        "print(\"Final Testing Accuracy: \", History.history['val_acc'][-1])"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Training Accuracy:  0.97314453125\n",
            "Final Testing Accuracy:  0.8815104166666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytpc51Xs6d8s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "583edfb6-7019-4773-a9af-64f8c339c184"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n",
        "ypred = model.predict(test_X,batch_size=256)\n",
        "ypred=np.argmax(ypred, axis=1)\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "print(classification_report(test_labels, ypred))\n"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        11\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       0.87      0.84      0.86       246\n",
            "           3       0.76      0.88      0.81       137\n",
            "           4       1.00      0.94      0.97        16\n",
            "           5       1.00      1.00      1.00         9\n",
            "           6       0.67      0.53      0.59        15\n",
            "           7       0.60      0.46      0.52        13\n",
            "           8       1.00      1.00      1.00        15\n",
            "           9       1.00      1.00      1.00        13\n",
            "          10       1.00      1.00      1.00        33\n",
            "          11       1.00      1.00      1.00        35\n",
            "          12       0.83      0.94      0.88        16\n",
            "          13       1.00      0.93      0.96        14\n",
            "          14       1.00      1.00      1.00         9\n",
            "          15       1.00      1.00      1.00        11\n",
            "          16       1.00      0.33      0.50         9\n",
            "          17       1.00      1.00      1.00        12\n",
            "          18       1.00      1.00      1.00        15\n",
            "          19       1.00      1.00      1.00         7\n",
            "          20       0.40      0.40      0.40        10\n",
            "          21       0.55      0.60      0.57        10\n",
            "          22       1.00      1.00      1.00        30\n",
            "          23       1.00      0.75      0.86         8\n",
            "          24       1.00      1.00      1.00        65\n",
            "\n",
            "    accuracy                           0.88       768\n",
            "   macro avg       0.91      0.86      0.88       768\n",
            "weighted avg       0.88      0.88      0.88       768\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoAmn-g-jmXb",
        "colab_type": "text"
      },
      "source": [
        "### **True Positive Rate (TPR)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jZUs4ndpajB",
        "colab_type": "code",
        "outputId": "65b4bf97-d62c-465f-f5d0-321531ee3816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(test_labels.shape)\n",
        "print(ypred.shape)\n",
        "\n",
        "cm = confusion_matrix(test_labels,ypred)\n",
        "print(cm)\n",
        "print(np.diag(cm))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768,)\n",
            "(768,)\n",
            "[[ 11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0 207  38   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   1   0   0   0   0]\n",
            " [  0   0  17 120   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   0   0  15   0   0   0   0   0   0   0   1   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   9   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   3   0   0   0   8   1   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   1   2   0   0   0]\n",
            " [  0   0   1   0   0   0   3   6   0   0   0   0   1   0   0   0   0   0\n",
            "    0   0   1   1   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  15   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  13   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  33   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  35   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   1   0   0   0   0   0   0   0   0   0  15   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1  13   0   0   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   9   0   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  11   0   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   6   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  12\n",
            "    0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   15   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   7   0   0   0   0   0]\n",
            " [  0   0   1   0   0   0   1   2   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   4   2   0   0   0]\n",
            " [  0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   3   6   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0  30   0   0]\n",
            " [  0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   6   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0  65]]\n",
            "[ 11   9 207 120  15   9   8   6  15  13  33  35  15  13   9  11   3  12\n",
            "  15   7   4   6  30   6  65]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJeYtIesloEZ",
        "colab_type": "code",
        "outputId": "f422cec7-07f8-4200-fdaf-305158b64cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "TotalTP , i = 0, 0\n",
        "while i <25:\n",
        "  TotalTP += np.diag(cm)[i]\n",
        "  i = i +1\n",
        "print(\"Total Number of Test Samples\",np.sum(cm))\n",
        "print(\"Number of True Positives\",TotalTP)\n",
        "print(\"Number of False Positives\",TotalTP)\n",
        "print(\"True Postive Rate\", TotalTP/np.sum(cm))\n"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of Test Samples 768\n",
            "Number of True Positives 677\n",
            "Number of False Positives 677\n",
            "True Postive Rate 0.8815104166666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4dQx0qi0cLm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d469f349-ff82-4d8d-a3ca-49d92bc0a02c"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(test_labels, ypred,pos_label=1)\n",
        "metrics.auc(fpr, tpr)\n"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01449275362318836"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RirtxGpr4vqG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7216a011-c82b-4412-b0ca-7fd389e38fd1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(tpr,fpr)\n",
        "plt.xlabel('True Positive Rate')\n",
        "plt.ylabel('False Positive Rate')\n",
        "plt.title('ROC Graph')\n",
        "plt.show()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMRQkSR1DQZLU+f+QndiarnHsbgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}