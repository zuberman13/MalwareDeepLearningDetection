{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Malware Detection Malimg",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalsadi/MalwareDeepLearningDetection/blob/master/Malware_Detection_Malimg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1lCK3oajsi0",
        "colab_type": "text"
      },
      "source": [
        "# **Malware Classification Using Deep Learning**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbGYpeYhuwK2",
        "colab_type": "text"
      },
      "source": [
        "This notebook will attempt to utilze state of the art neural network architectures to further develop the accuracy achieved by researchers on this topic. Dataset information and network descriptions will be provided below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uria_HHzoGF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras \n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7EOkalP06aw",
        "colab_type": "text"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIWyLUVQkAdW",
        "colab_type": "text"
      },
      "source": [
        "## **Dataset**\n",
        "The malware dataset was obtained courtsey of Vision Research Lab\n",
        "University of California, Santa Barbara "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knjbVN7-obnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = np.load('/content/malimg.npz',allow_pickle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNV9eGoxo7PD",
        "colab_type": "code",
        "outputId": "73ddc610-2fd4-4b27-b40f-817c6b469fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "BATCH_SIZE = 256 \n",
        "CELL_SIZE = 256 \n",
        "DROPOUT_RATE = 0.85 \n",
        "LEARNING_RATE = 1e-3 \n",
        "NODE_SIZE = [512, 256, 128] \n",
        "NUM_LAYERS = 5\n",
        "\n",
        "features = dataset['arr'][:, 0]\n",
        "features = np.array([feature for feature in features])\n",
        "features = np.reshape(features, (features.shape[0], features.shape[1] * features.shape[2]))\n",
        "r, c = features.shape\n",
        "\n",
        "print(\"Number of Samples\" , r)\n",
        "print(\"Number of Features\" , c)\n",
        "\n",
        "if 1==1:\n",
        "    features = StandardScaler().fit_transform(features)\n",
        "\n",
        "    \n",
        "labels = dataset['arr'][:, 1]\n",
        "labels = np.array([label for label in labels])\n",
        "\n",
        "\n",
        "one_hot = np.zeros((labels.shape[0], labels.max() + 1))\n",
        "one_hot[np.arange(labels.shape[0]), labels] = 1\n",
        "labels = one_hot\n",
        "labels[labels == 0] = 0\n",
        "num_features = features.shape[1]\n",
        "num_classes = labels.shape[1]\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.10,\n",
        "                                                                                stratify=labels)\n",
        "\n",
        "train_size = int(train_features.shape[0])\n",
        "train_features = train_features[:train_size-(train_size % BATCH_SIZE)]\n",
        "train_labels = train_labels[:train_size-(train_size % BATCH_SIZE)]\n",
        "\n",
        "test_size = int(test_features.shape[0])\n",
        "test_features = test_features[:test_size - (test_size % BATCH_SIZE)]\n",
        "test_labels = test_labels[:test_size - (test_size % BATCH_SIZE)]\n",
        "\n",
        "\n",
        "\n",
        "r, c = train_features.shape\n",
        "print(\"Number of Training Samples\" , r)\n",
        "print(\"Number of Training Features\" , c)\n",
        "\n",
        "r, c = test_features.shape\n",
        "print(\"Number of Test Samples\" , r)\n",
        "print(\"Number of Test Features\" , c)\n",
        "#print(train_labels.shape)\n",
        "\n",
        "#print(tf.reshape(test_features[1], [32,32]))\n",
        "\n",
        "#print(train_features.shape,test_features.shape, train_labels.shape, test_labels.shape )\n",
        "\n",
        "#print(train_labels)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples 9339\n",
            "Number of Features 1024\n",
            "Number of Training Samples 8192\n",
            "Number of Training Features 1024\n",
            "Number of Test Samples 768\n",
            "Number of Test Features 1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sCIo80sTqxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras as k\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "import sys\n",
        "  \n",
        "train_X = train_features.reshape(-1, 32,32, 1)\n",
        "test_X = test_features.reshape(-1, 32,32, 1)  \n",
        "\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "num_classes = 25\n",
        "\n",
        "input_shape = (32, 32, 1)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRuTA5kyUcxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def specificity(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    param:\n",
        "    y_pred - Predicted labels\n",
        "    y_true - True labels \n",
        "    Returns:\n",
        "    Specificity score\n",
        "    \"\"\"\n",
        "    neg_y_true = 1 - y_true\n",
        "    neg_y_pred = 1 - y_pred\n",
        "    fp = np.sum(neg_y_true * y_pred)\n",
        "    tn = np.sum(neg_y_true * neg_y_pred)\n",
        "    specificity = tn / (tn + fp + sys.float_info.epsilon)\n",
        "    return specificity\n",
        "\n",
        "def FPR(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    param:\n",
        "    y_pred - Predicted labels\n",
        "    y_true - True labels \n",
        "    Returns:\n",
        "    Specificity score\n",
        "    \"\"\"\n",
        "    neg_y_true = 1 - y_true\n",
        "    neg_y_pred = 1 - y_pred\n",
        "    fp = np.sum(neg_y_true * y_pred)\n",
        "    tn = np.sum(neg_y_true * neg_y_pred)\n",
        "    specificity = tn / (tn + fp + sys.float_info.epsilon)\n",
        "    return  1 - specificity\n",
        "  \n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=k.optimizers.Adam(),metrics=['accuracy',specificity, FPR])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PcOuV-HUeSY",
        "colab_type": "code",
        "outputId": "896b40dd-18c2-4625-8638-0e66cc9c6b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#print(train_labels)\n",
        "#print(test_labels)\n",
        "#20\n",
        "History = model.fit(train_X, train_labels, batch_size=batch_size,epochs=20,verbose=1,validation_data=(test_X, test_labels))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8192/8192 [==============================] - 2s 241us/step - loss: 2.5081 - acc: 0.3151 - specificity: 0.9259 - FPR: 0.0741 - val_loss: 2.5183 - val_acc: 0.3281 - val_specificity: 0.9256 - val_FPR: 0.0744\n",
            "Epoch 5/20\n",
            "8192/8192 [==============================] - 2s 241us/step - loss: 2.4657 - acc: 0.3167 - specificity: 0.9262 - FPR: 0.0738 - val_loss: 2.5450 - val_acc: 0.3190 - val_specificity: 0.9257 - val_FPR: 0.0743\n",
            "Epoch 6/20\n",
            "8192/8192 [==============================] - 2s 237us/step - loss: 2.4153 - acc: 0.3256 - specificity: 0.9266 - FPR: 0.0734 - val_loss: 2.5598 - val_acc: 0.3034 - val_specificity: 0.9258 - val_FPR: 0.0742\n",
            "Epoch 7/20\n",
            "8192/8192 [==============================] - 2s 241us/step - loss: 2.3496 - acc: 0.3356 - specificity: 0.9271 - FPR: 0.0729 - val_loss: 2.5848 - val_acc: 0.3034 - val_specificity: 0.9258 - val_FPR: 0.0742\n",
            "Epoch 8/20\n",
            "8192/8192 [==============================] - 2s 240us/step - loss: 2.2723 - acc: 0.3513 - specificity: 0.9278 - FPR: 0.0722 - val_loss: 2.5911 - val_acc: 0.3034 - val_specificity: 0.9254 - val_FPR: 0.0746\n",
            "Epoch 9/20\n",
            "8192/8192 [==============================] - 2s 243us/step - loss: 2.1713 - acc: 0.3760 - specificity: 0.9287 - FPR: 0.0713 - val_loss: 2.6521 - val_acc: 0.2878 - val_specificity: 0.9261 - val_FPR: 0.0739\n",
            "Epoch 10/20\n",
            "8192/8192 [==============================] - 2s 239us/step - loss: 2.0782 - acc: 0.3925 - specificity: 0.9295 - FPR: 0.0705 - val_loss: 2.6701 - val_acc: 0.2852 - val_specificity: 0.9256 - val_FPR: 0.0744\n",
            "Epoch 11/20\n",
            "8192/8192 [==============================] - 2s 239us/step - loss: 1.9858 - acc: 0.4214 - specificity: 0.9304 - FPR: 0.0696 - val_loss: 2.7519 - val_acc: 0.2812 - val_specificity: 0.9264 - val_FPR: 0.0736\n",
            "Epoch 12/20\n",
            "8192/8192 [==============================] - 2s 237us/step - loss: 1.9059 - acc: 0.4366 - specificity: 0.9313 - FPR: 0.0687 - val_loss: 2.8270 - val_acc: 0.2747 - val_specificity: 0.9264 - val_FPR: 0.0736\n",
            "Epoch 13/20\n",
            "8192/8192 [==============================] - 2s 240us/step - loss: 1.8115 - acc: 0.4597 - specificity: 0.9323 - FPR: 0.0677 - val_loss: 2.8822 - val_acc: 0.2695 - val_specificity: 0.9264 - val_FPR: 0.0736\n",
            "Epoch 14/20\n",
            "8192/8192 [==============================] - 2s 239us/step - loss: 1.7356 - acc: 0.4912 - specificity: 0.9331 - FPR: 0.0669 - val_loss: 2.8709 - val_acc: 0.2669 - val_specificity: 0.9263 - val_FPR: 0.0737\n",
            "Epoch 15/20\n",
            "8192/8192 [==============================] - 2s 242us/step - loss: 1.6710 - acc: 0.5016 - specificity: 0.9339 - FPR: 0.0661 - val_loss: 2.9616 - val_acc: 0.2669 - val_specificity: 0.9266 - val_FPR: 0.0734\n",
            "Epoch 16/20\n",
            "8192/8192 [==============================] - 2s 250us/step - loss: 1.5954 - acc: 0.5284 - specificity: 0.9349 - FPR: 0.0651 - val_loss: 3.0455 - val_acc: 0.2786 - val_specificity: 0.9268 - val_FPR: 0.0732\n",
            "Epoch 17/20\n",
            "8192/8192 [==============================] - 2s 247us/step - loss: 1.5556 - acc: 0.5371 - specificity: 0.9355 - FPR: 0.0645 - val_loss: 3.0581 - val_acc: 0.2799 - val_specificity: 0.9268 - val_FPR: 0.0732\n",
            "Epoch 18/20\n",
            "8192/8192 [==============================] - 2s 241us/step - loss: 1.5035 - acc: 0.5530 - specificity: 0.9361 - FPR: 0.0639 - val_loss: 3.1915 - val_acc: 0.2826 - val_specificity: 0.9274 - val_FPR: 0.0726\n",
            "Epoch 19/20\n",
            "8192/8192 [==============================] - 2s 242us/step - loss: 1.4491 - acc: 0.5688 - specificity: 0.9369 - FPR: 0.0631 - val_loss: 3.2350 - val_acc: 0.2839 - val_specificity: 0.9274 - val_FPR: 0.0726\n",
            "Epoch 20/20\n",
            "8192/8192 [==============================] - 2s 240us/step - loss: 1.4114 - acc: 0.5837 - specificity: 0.9374 - FPR: 0.0626 - val_loss: 3.2478 - val_acc: 0.2721 - val_specificity: 0.9273 - val_FPR: 0.0727\n",
            "Train on 8192 samples, validate on 768 samples\n",
            "Epoch 1/20\n",
            "8192/8192 [==============================] - 2s 301us/step - loss: 1.3787 - acc: 0.5925 - specificity: 0.9380 - FPR: 0.0620 - val_loss: 0.6663 - val_acc: 0.7969 - val_specificity: 0.9468 - val_FPR: 0.0532\n",
            "Epoch 2/20\n",
            "8192/8192 [==============================] - 2s 239us/step - loss: 0.6519 - acc: 0.7841 - specificity: 0.9473 - FPR: 0.0527 - val_loss: 0.4895 - val_acc: 0.8255 - val_specificity: 0.9496 - val_FPR: 0.0504\n",
            "Epoch 3/20\n",
            "8192/8192 [==============================] - 2s 237us/step - loss: 0.4668 - acc: 0.8397 - specificity: 0.9502 - FPR: 0.0498 - val_loss: 0.4229 - val_acc: 0.8607 - val_specificity: 0.9514 - val_FPR: 0.0486\n",
            "Epoch 4/20\n",
            "8192/8192 [==============================] - 2s 237us/step - loss: 0.3350 - acc: 0.8820 - specificity: 0.9526 - FPR: 0.0474 - val_loss: 0.3856 - val_acc: 0.8672 - val_specificity: 0.9524 - val_FPR: 0.0476\n",
            "Epoch 5/20\n",
            "8192/8192 [==============================] - 2s 237us/step - loss: 0.2601 - acc: 0.9062 - specificity: 0.9540 - FPR: 0.0460 - val_loss: 0.4017 - val_acc: 0.8529 - val_specificity: 0.9533 - val_FPR: 0.0467\n",
            "Epoch 6/20\n",
            "8192/8192 [==============================] - 2s 236us/step - loss: 0.1932 - acc: 0.9292 - specificity: 0.9554 - FPR: 0.0446 - val_loss: 0.3754 - val_acc: 0.8646 - val_specificity: 0.9536 - val_FPR: 0.0464\n",
            "Epoch 7/20\n",
            "8192/8192 [==============================] - 2s 237us/step - loss: 0.1597 - acc: 0.9425 - specificity: 0.9562 - FPR: 0.0438 - val_loss: 0.3835 - val_acc: 0.8698 - val_specificity: 0.9541 - val_FPR: 0.0459\n",
            "Epoch 8/20\n",
            "8192/8192 [==============================] - 2s 235us/step - loss: 0.1289 - acc: 0.9563 - specificity: 0.9570 - FPR: 0.0430 - val_loss: 0.3620 - val_acc: 0.8685 - val_specificity: 0.9543 - val_FPR: 0.0457\n",
            "Epoch 9/20\n",
            "8192/8192 [==============================] - 2s 238us/step - loss: 0.1042 - acc: 0.9655 - specificity: 0.9575 - FPR: 0.0425 - val_loss: 0.4114 - val_acc: 0.8724 - val_specificity: 0.9547 - val_FPR: 0.0453\n",
            "Epoch 10/20\n",
            "8192/8192 [==============================] - 2s 239us/step - loss: 0.0979 - acc: 0.9667 - specificity: 0.9577 - FPR: 0.0423 - val_loss: 0.4582 - val_acc: 0.8672 - val_specificity: 0.9544 - val_FPR: 0.0456\n",
            "Epoch 11/20\n",
            "8192/8192 [==============================] - 2s 237us/step - loss: 0.0790 - acc: 0.9752 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.4665 - val_acc: 0.8737 - val_specificity: 0.9548 - val_FPR: 0.0452\n",
            "Epoch 12/20\n",
            "8192/8192 [==============================] - 2s 236us/step - loss: 0.0841 - acc: 0.9713 - specificity: 0.9581 - FPR: 0.0419 - val_loss: 0.4588 - val_acc: 0.8854 - val_specificity: 0.9550 - val_FPR: 0.0450\n",
            "Epoch 13/20\n",
            "8192/8192 [==============================] - 2s 236us/step - loss: 0.0807 - acc: 0.9730 - specificity: 0.9582 - FPR: 0.0418 - val_loss: 0.4530 - val_acc: 0.8815 - val_specificity: 0.9549 - val_FPR: 0.0451\n",
            "Epoch 14/20\n",
            "8192/8192 [==============================] - 2s 241us/step - loss: 0.0791 - acc: 0.9720 - specificity: 0.9582 - FPR: 0.0418 - val_loss: 0.5566 - val_acc: 0.8841 - val_specificity: 0.9551 - val_FPR: 0.0449\n",
            "Epoch 15/20\n",
            "8192/8192 [==============================] - 2s 240us/step - loss: 0.0620 - acc: 0.9796 - specificity: 0.9586 - FPR: 0.0414 - val_loss: 0.4855 - val_acc: 0.8880 - val_specificity: 0.9551 - val_FPR: 0.0449\n",
            "Epoch 16/20\n",
            "8192/8192 [==============================] - 2s 238us/step - loss: 0.0613 - acc: 0.9800 - specificity: 0.9586 - FPR: 0.0414 - val_loss: 0.4852 - val_acc: 0.8867 - val_specificity: 0.9551 - val_FPR: 0.0449\n",
            "Epoch 17/20\n",
            "8192/8192 [==============================] - 2s 237us/step - loss: 0.0593 - acc: 0.9811 - specificity: 0.9587 - FPR: 0.0413 - val_loss: 0.5614 - val_acc: 0.8724 - val_specificity: 0.9548 - val_FPR: 0.0452\n",
            "Epoch 18/20\n",
            "8192/8192 [==============================] - 2s 238us/step - loss: 0.0604 - acc: 0.9794 - specificity: 0.9587 - FPR: 0.0413 - val_loss: 0.5103 - val_acc: 0.8737 - val_specificity: 0.9549 - val_FPR: 0.0451\n",
            "Epoch 19/20\n",
            "8192/8192 [==============================] - 2s 235us/step - loss: 0.0578 - acc: 0.9814 - specificity: 0.9588 - FPR: 0.0412 - val_loss: 0.4829 - val_acc: 0.8880 - val_specificity: 0.9553 - val_FPR: 0.0447\n",
            "Epoch 20/20\n",
            "8192/8192 [==============================] - 2s 236us/step - loss: 0.0562 - acc: 0.9816 - specificity: 0.9588 - FPR: 0.0412 - val_loss: 0.5400 - val_acc: 0.8737 - val_specificity: 0.9547 - val_FPR: 0.0453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfWecsjkjZkU",
        "colab_type": "text"
      },
      "source": [
        "## **Performance Metrics**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yelpg-EQ6UlZ",
        "colab_type": "text"
      },
      "source": [
        "### **Accuracy and Precison**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcDT4TiPAF5N",
        "colab_type": "code",
        "outputId": "7f61556b-171e-4afa-c116-7c708a913dc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Final Training Accuracy: \", History.history['acc'][-1])\n",
        "print(\"Final Testing Accuracy: \", History.history['val_acc'][-1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Training Accuracy:  0.9815673828125\n",
            "Final Testing Accuracy:  0.8736979166666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytpc51Xs6d8s",
        "colab_type": "code",
        "outputId": "4691e4fe-4dd3-450d-ec9b-abbef16b8f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n",
        "ypred = model.predict(test_X,batch_size=256)\n",
        "ypred=np.argmax(ypred, axis=1)\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "print(classification_report(test_labels, ypred))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         8\n",
            "           2       0.84      0.90      0.87       243\n",
            "           3       0.79      0.84      0.81       129\n",
            "           4       0.92      0.85      0.88        13\n",
            "           5       1.00      1.00      1.00         8\n",
            "           6       0.67      0.32      0.43        19\n",
            "           7       0.26      0.45      0.33        11\n",
            "           8       1.00      1.00      1.00        13\n",
            "           9       1.00      1.00      1.00        12\n",
            "          10       1.00      1.00      1.00        32\n",
            "          11       1.00      1.00      1.00        34\n",
            "          12       1.00      0.76      0.87        17\n",
            "          13       1.00      1.00      1.00        16\n",
            "          14       1.00      1.00      1.00        11\n",
            "          15       0.92      0.92      0.92        12\n",
            "          16       1.00      0.17      0.29        12\n",
            "          17       1.00      1.00      1.00        12\n",
            "          18       1.00      1.00      1.00        12\n",
            "          19       1.00      1.00      1.00         5\n",
            "          20       0.67      0.33      0.44        12\n",
            "          21       0.58      0.64      0.61        11\n",
            "          22       1.00      1.00      1.00        37\n",
            "          23       1.00      0.70      0.82        10\n",
            "          24       1.00      1.00      1.00        69\n",
            "\n",
            "    accuracy                           0.87       768\n",
            "   macro avg       0.91      0.83      0.85       768\n",
            "weighted avg       0.88      0.87      0.87       768\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoAmn-g-jmXb",
        "colab_type": "text"
      },
      "source": [
        "### **True Positive Rate (TPR)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jZUs4ndpajB",
        "colab_type": "code",
        "outputId": "e8cb605b-c668-4d7f-daeb-efc8cb6b5d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(test_labels.shape)\n",
        "print(ypred.shape)\n",
        "\n",
        "cm = confusion_matrix(test_labels,ypred)\n",
        "print(np.diag(cm))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768,)\n",
            "(768,)\n",
            "[ 10   8 218 108  11   8   6   5  13  12  32  34  13  16  11  11   2  12\n",
            "  12   5   4   7  37   7  69]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJeYtIesloEZ",
        "colab_type": "code",
        "outputId": "b3a7f663-9d3f-4763-e253-66b3a20c8ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "TotalTP , i = 0, 0\n",
        "while i <25:\n",
        "  TotalTP += np.diag(cm)[i]\n",
        "  i = i +1\n",
        "print(\"Total Number of Test Samples\",np.sum(cm))\n",
        "print(\"Number of True Positives\",TotalTP)\n",
        "print(\"Number of False Positives\",TotalTP)\n",
        "print(\"True Postive Rate\", TotalTP/np.sum(cm))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of Test Samples 768\n",
            "Number of True Positives 671\n",
            "Number of False Positives 671\n",
            "True Postive Rate 0.8736979166666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4dQx0qi0cLm",
        "colab_type": "code",
        "outputId": "e206d99e-a3ac-4c61-9a27-a87563474aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(test_labels, ypred,pos_label=1)\n",
        "metrics.auc(fpr, tpr)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.013157894736842146"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RirtxGpr4vqG",
        "colab_type": "code",
        "outputId": "90652e91-fefd-4f8a-a9bf-2c65e80e849e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(tpr,fpr)\n",
        "plt.xlabel('True Positive Rate')\n",
        "plt.ylabel('False Positive Rate')\n",
        "plt.title('ROC Graph')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMRQkSR1DQZLU+f+QndiarnHsbgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}